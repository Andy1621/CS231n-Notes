# CS231n

### 课程资源

- 原课程：<http://cs231n.stanford.edu/>

- AI研习社视频：<https://ai.yanxishe.com/page/groupDetail/19>
- 官网笔记：<http://cs231n.github.io/>
- 知乎翻译笔记：<https://zhuanlan.zhihu.com/p/21930884>
- 作业代码：<https://github.com/lightaime/cs231n>、<https://github.com/Halfish/cs231n>

**Try your best and let it be!**

## 课程笔记

### Leture1

1. 计算机视觉顾名思义就是针对视觉数据的研究

2. 视觉数据成为网络传输数据的主要部分

3. 跨学科

   ![](./pic/1.jpg)

4. 视觉处理源自视觉世界的简单结构——面向边缘

5. 从图像中构造出视觉世界中最终全面的3D表现

   ![](./pic/2.jpg)

6. 广义结构体、图形结构：将物体复杂结构简化成一个有更简单形状和几何结构的几何体

7. 目标识别=>目标分割

8. 目标识别的首要任务是在目标上确认这些关键的特征，然后把这些特征与相似的目标进行匹配，这比直接匹配整个目标要容易得多

9. 方向梯度直方图、可变部件模型

10. 在21世纪早期才开始真正拥有标注的数据集=>目标识别成为非常基本的问题

11. 2012年卷积神经网络的使用使ImageNet比赛误差率大大降低

12. CS231n 关注目标识别——图像分类问题，其他相关问题如目标检测或图像摘要生成

13. 2012AlexNet，2014GoogleNet、VGG，2015ResNet

14. 卷积90s提出，但直到2012才迅速发展，与计算能力的增长（摩尔定律、GPU并行运算）以及大规模高质量的数据集离不开

### Leture2

1. 图片分类，挑战：语义鸿沟、视角、光照、变形、遮挡、背景、类内误差

2. L1曼哈顿距离，坐标差值的绝对值之和，决定于坐标系的选择

   L2欧氏距离，坐标差值平方和开方，不关心坐标系的选择

   K近邻算法：选取最近的K个投票决定类型，关键是确定距离的计算公式

3. 将数据分为训练集、验证集和测试集

   交叉验证集，将数据分为训练集和测试集，训练集中轮流选一部分作为验证集（一般分为3、5、10份）

4. K近邻在图像任务中永远不会用到，L1、L2距离并不能很好地反映图像特征

5. **线性分类器** 
   $$
   f=(x_i, W, b)=Wx_i+b
   $$
   在$$x_i$$中加一个维度，值为常量1，即可将偏差和权重合并

6. 图像预处理经常需要**零均值中心化**，即减去平均值除以分布区间得到$$[-1,1]$$分布

7. **多类支持向量机损失**
   $$
   L_i = \sum_{j\neq y_i}max(0, s_j-s_{y_i}+\Delta)
   $$
   $$max(0,-)$$常被称为折叶损失**hinge loss**，

   平方折叶损失SVM（L2-SVM）为$$max(0, -)^2$$

   平方折叶损失将更强烈地惩罚过界的边界值，不使用平方是更标准的版本，但在某些数据集中，平方折叶损失会工作得更好，可以通过交叉验证来决定到底使用哪个

   ![](./pic/3.jpg)

   多类SVM的目标是正确类别的分类比其他不正确类别的分类的分数要高，而且至少高出$$\Delta$$的边界值，如果其他分类分数进入了红色的区域，甚至更高，那么就开始计算损失。如果没有这些情况，损失值为0。我们的目标是找到一些权重，它们既能够让训练集中的数据样例满足这些限制，也能让总的损失值尽可能地低。

8. **正则化惩罚**（常用L2范式）
   $$
   R(W) = \sum_k\sum_lW^2_{k, l}
   $$
   引入正则化项可以对大数值权重进行惩罚，提高其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。L2惩罚倾向于更小更分散的权重向量，这就会鼓励分类器最终将所有维度上的特征都用起来，而不是强烈依赖其中少数几个维度。在后面的课程中可以看到，这一效果将会提升分类器的泛化能力，并避免**过拟合**。

9. 完整多类SVM损失函数
   $$
    L= \frac{1}{N}\sum_iL_i + \lambda R(W)
   $$

10. **Softmax分类器**
    **交叉熵损失**
    $$
    L_i=-log(\frac{e^{f_{y_i}}}{\sum_je^{f_j}})
    $$
    在计算时中间项可能数值非常大，通常可在分子分母同乘以常数$$C, logC=-max_jf_j$$，就是讲向量$$f$$中的数值进行平移，使得最大值为0

11. 通常来说两种分类器的表现差别很小，相对于Softmax分类器，SVM更加**”局部目标化“**，如第一个分类是正确的，$$[10, -100, -100]$$或者$$[10, 9, 9]$$对SVM来说没有不同。

    但对于Softmax分类器，前者的损失值远高于后者，换言之Softmax分类器对于分数是永远不会满意的：**正确分类总能得到更高的可能性，错误分类总能得到更低的可能性，损失值总是能够更小**。但SVM只要边界值被满足了就满意了，不会超过限制去细微地操作具体分数。这可以被看做是SVM的一种特性。举例说来，一个汽车的分类器应该把他的大量精力放在如何分辨小轿车和大卡车上，而不应该纠结于如何与青蛙进行区分，因为区分青蛙得到的评分已经足够低了。

    